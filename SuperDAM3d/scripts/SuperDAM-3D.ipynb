{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import trange, tqdm\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import utilities as UT\n",
    "from ranksvm import get_dynamic_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utilities as UT\n",
    "from ranksvm import get_dynamic_image\n",
    "\n",
    "LABEL_PATH = '/home/raytrack/.jupyter/Dynamic/Preprocessed'\n",
    "\n",
    "def prep_data(LABEL_PATH, TEST_NUM):\n",
    "    # This function is used to prepare train/test labels for 5-fold cross-validation\n",
    "    TEST_LABEL = f'{LABEL_PATH}/fold_CNvsAD_{TEST_NUM}.csv'\n",
    "\n",
    "    # combine train labels\n",
    "    filenames = [f'{LABEL_PATH}/fold_CNvsAD_{i}.csv' for i in range(5)]\n",
    "    filenames.remove(TEST_LABEL)\n",
    "\n",
    "    combined_train_list_path = f'{LABEL_PATH}/combined_train_list_{TEST_NUM}.csv'\n",
    "    with open(combined_train_list_path, 'w') as combined_train_list:\n",
    "        for fold in filenames:\n",
    "            for line in open(fold, 'r'):\n",
    "                combined_train_list.write(line)\n",
    "    TRAIN_LABEL = combined_train_list_path\n",
    "    \n",
    "    return TRAIN_LABEL, TEST_LABEL\n",
    "\n",
    "\n",
    "class Dataset_Early_Fusion(Dataset):\n",
    "    def __init__(self, label_file):\n",
    "        self.files = UT.read_csv(label_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        temp = self.files[idx]\n",
    "        full_path = temp[0]\n",
    "\n",
    "        label_str = full_path.split('/')[-3]  # Get the label string from the file path\n",
    "        if label_str == 'CN':\n",
    "            label = 0\n",
    "        elif label_str == 'AD':\n",
    "            label = 1\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected label: {label_str}')\n",
    "\n",
    "        im = np.load(full_path)\n",
    "       #  im = np.load(full_path, allow_pickle=True)\n",
    "        im = get_dynamic_image(im)\n",
    "        im = np.expand_dims(im, 0)\n",
    "        im = np.concatenate([im, im, im], 0)\n",
    "\n",
    "        return im, label, full_path  # label is now an int\n",
    "\n",
    "    \n",
    "        print(full_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 110, 110])\n",
      "Model output shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DGM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4, groups=4):  # reduction 默认值从 16 改为 4\n",
    "        super(DGM, self).__init__()\n",
    "        self.groups = groups\n",
    "        mid_channels = max(1, in_channels // reduction)  # 保证 mid_channels > 0 且与 groups 兼容\n",
    "\n",
    "        # 动态权重调整层\n",
    "        self.weight_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, groups=self.groups, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, in_channels, 1, groups=self.groups, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # 分组点卷积\n",
    "        self.pointwise_groups = nn.Conv2d(in_channels, in_channels, 1, groups=self.groups, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 动态权重\n",
    "        weights = self.weight_layer(x)\n",
    "        # 应用动态权重\n",
    "        x = x * weights\n",
    "        # 分组点卷积进行特征融合\n",
    "        x = self.pointwise_groups(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# 定义全局注意力模块（GAM）\n",
    "class GAM_Attention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GAM_Attention, self).__init__()\n",
    "\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)  # 全局平均池化层\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // 16),  # 线性变换层，减小通道数\n",
    "            nn.ReLU(inplace=True),  # ReLU 激活函数\n",
    "            nn.Linear(in_channels // 16, in_channels),  # 线性变换层，恢复通道数\n",
    "            nn.Sigmoid()  # Sigmoid 激活函数，产生通道注意力权重\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # 全局平均池化，将特征图变成全局平均值\n",
    "        x_global = self.global_avgpool(x).view(b, c)\n",
    "\n",
    "        # 通道注意力：通过线性变换和 Sigmoid 操作产生通道权重\n",
    "        x_channel_att = self.channel_attention(x_global).view(b, c, 1, 1)\n",
    "\n",
    "        # 将输入特征图按通道加权\n",
    "        x = x * x_channel_att\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定义 LinearBottleNeck_1 模块\n",
    "class LinearBottleNeck_1(nn.Module):\n",
    "    def __init__(self, in_c, out_c, s, t):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_c, in_c * t, 1),  # 1x1 卷积层，升维操作\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 3, stride=s, padding=1, groups=in_c * t),  # 3x3 深度可分离卷积\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 1, stride=1, padding=0, groups=1),  # 1x1 卷积层\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "\n",
    "            nn.Conv2d(in_c * t, out_c, 1),  # 1x1 卷积层，降维操作\n",
    "            nn.BatchNorm2d(out_c)  # 批归一化\n",
    "        )\n",
    "\n",
    "        self.stride = s  # 步长\n",
    "        self.in_channels = in_c  # 输入通道数\n",
    "        self.out_channels = out_c  # 输出通道数\n",
    "\n",
    "        # 添加全局注意力模块\n",
    "        self.attention = GAM_Attention(out_c)  # 使用定义的全局注意力模块\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x  # 恒等映射，如果步长为1且通道数不变，则加上原始输入\n",
    "\n",
    "        # 应用全局注意力\n",
    "        residual = self.attention(residual)\n",
    "\n",
    "        return residual\n",
    "\n",
    "\n",
    "# 定义 LinearBottleNeck_2 模块\n",
    "class LinearBottleNeck_2(nn.Module):\n",
    "    def __init__(self, in_c, out_c, s, t):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_c, in_c * t, 1),  # 1x1 卷积层，升维操作\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 3, stride=s, padding=1, groups=in_c * t),  # 3x3 深度可分离卷积\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 1, stride=1, padding=0, groups=1),  # 1x1 卷积层\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "\n",
    "            nn.Conv2d(in_c * t, out_c, 1),  # 1x1 卷积层，降维操作\n",
    "            nn.BatchNorm2d(out_c)  # 批归一化\n",
    "        )\n",
    "\n",
    "        self.residual_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, in_c * t, 1),  # 1x1 卷积层，升维操作\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 5, stride=s, padding=2, groups=in_c * t),  # 5x5 深度可分离卷积\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "            nn.ReLU6(inplace=True),  # ReLU6 激活函数\n",
    "\n",
    "            nn.Conv2d(in_c * t, in_c * t, 1, stride=1, padding=0, groups=1),  # 1x1 卷积层\n",
    "            nn.BatchNorm2d(in_c * t),  # 批归一化\n",
    "\n",
    "            nn.Conv2d(in_c * t, out_c, 1),  # 1x1 卷积层，降维操作\n",
    "            nn.BatchNorm2d(out_c)  # 批归一化\n",
    "        )\n",
    "\n",
    "        self.residual_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, 1, stride=2),  # 1x1 卷积层，步长为2，降维操作\n",
    "            nn.BatchNorm2d(out_c)  # 批归一化\n",
    "        )\n",
    "\n",
    "        self.stride = s  # 步长\n",
    "        self.in_channels = in_c  # 输入通道数\n",
    "        self.out_channels = out_c  # 输出通道数\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        residual_1 = self.residual_1(x)\n",
    "        residual_2 = self.residual_2(x)\n",
    "\n",
    "        # 多尺度特征融合\n",
    "        out_feature = residual_1 + residual + residual_2\n",
    "\n",
    "        return out_feature\n",
    "\n",
    "# 定义 SuperDAM\n",
    "class SuperDAM(nn.Module):\n",
    "    def __init__(self, class_num=2):\n",
    "        super().__init__()\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            LinearBottleNeck_1(32, 16, 1, 1),\n",
    "            DGM(16)  # 在 stage1 后加入 DGM\n",
    "        )\n",
    "        self.stage2 = self.make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self.make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = nn.Sequential(\n",
    "            self.make_stage(4, 32, 64, 2, 6),\n",
    "            DGM(64)  # 在 stage4 后加入  DGM\n",
    "        )\n",
    "        self.stage5 = self.make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = nn.Sequential(\n",
    "            self.make_stage(3, 96, 160, 2, 6),\n",
    "            DGM(160)  # 在 stage6 后加入 DGM\n",
    "        )\n",
    "        self.stage7 = LinearBottleNeck_1(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            DGM(1280)  # 在 conv1 后加入  DGM\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def make_stage(self, repeat, in_c, out_c, s, t):\n",
    "        layers = []\n",
    "        if s == 1:\n",
    "            layers.append(LinearBottleNeck_1(in_c, out_c, s, t))\n",
    "        else:\n",
    "            layers.append(LinearBottleNeck_2(in_c, out_c, s, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck_1(out_c, out_c, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义测试的输入数据\n",
    "    batch_size = 4\n",
    "    sample_data = torch.rand(batch_size, 3, 110, 110)  # 输入数据形状为 (batch_size, 3, 110, 110)\n",
    "\n",
    "    # 初始化模型\n",
    "    model =SuperDAM (class_num=2)  # 类别数为2\n",
    "\n",
    "    # 测试模型输出\n",
    "    output = model(sample_data)\n",
    "\n",
    "    # 打印模型输出形状\n",
    "    print(f\"Input shape: {sample_data.shape}\")  # 输入形状\n",
    "    print(f\"Model output shape: {output.shape}\")  # 输出形状，期望是 (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader):\n",
    "# Assuming 'net' is your model instance\n",
    "    # 检查是否有可用的 GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = SuperDAM( class_num=2)\n",
    "    net.to(device)\n",
    "    # 加载预训练权重\n",
    "    pretrained_weights_path = '/home/raytrack/.jupyter/Dynamic/newmodel_weights.pth'\n",
    "    pretrained_dict = torch.load(pretrained_weights_path)\n",
    "\n",
    "    # 获取模型的现有权重字典\n",
    "    model_dict = net.state_dict()\n",
    "\n",
    "    # 过滤掉不匹配的权重\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "\n",
    "    # 更新现有的模型权重字典\n",
    "    model_dict.update(pretrained_dict)\n",
    "\n",
    "    # 加载过滤后的权重字典\n",
    "    net.load_state_dict(model_dict, strict=False)\n",
    "    \n",
    "    #opt = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=0.001)\n",
    "    #opt = torch.optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "    #scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma= 0.985)\n",
    "    # scheduler = torch.optim.lr_scheduler.CyclicLR(opt, \n",
    "    #                                               base_lr=LR, \n",
    "    #                                               max_lr=0.001, \n",
    "    #                                               step_size_up=100,\n",
    "    #                                               cycle_momentum=False)\n",
    "    opt  = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=50)\n",
    "    LOSS_WEIGHTS = torch.tensor([1., 1.]) \n",
    "    loss_fcn = torch.nn.CrossEntropyLoss(weight=LOSS_WEIGHTS.to(device))\n",
    "    #loss_fcn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0]).to(device))    \n",
    "    t = trange(EPOCHS, desc=' ', leave=True)\n",
    "\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "    pred_result = []\n",
    "    old_acc = 0\n",
    "    old_auc = 0\n",
    "    test_acc = 0\n",
    "    best_epoch = 0\n",
    "    test_performance = []\n",
    "    for e in t:    \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        val_y_true = []\n",
    "        val_y_pred = []                \n",
    "        \n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        # training\n",
    "        net.train()\n",
    "        for step, (img, label, _) in enumerate(train_dataloader):\n",
    "            img = img.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "            opt.zero_grad()\n",
    "            out = net(img)\n",
    "            loss = loss_fcn(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            label = label.cpu().detach()\n",
    "            out = out.cpu().detach()\n",
    "            y_true, y_pred = UT.assemble_labels(step, y_true, y_pred, label, out)        \n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss/(step+1)\n",
    "        acc = float(torch.sum(torch.max(y_pred, 1)[1]==y_true))/ float(len(y_pred))\n",
    "        auc = metrics.roc_auc_score(y_true, y_pred[:,1])\n",
    "        f1 = metrics.f1_score(y_true, torch.max(y_pred, 1)[1])\n",
    "        precision = metrics.precision_score(y_true, torch.max(y_pred, 1)[1])\n",
    "        recall = metrics.recall_score(y_true, torch.max(y_pred, 1)[1])\n",
    "        ap = metrics.average_precision_score(y_true, torch.max(y_pred, 1)[1]) #average_precision\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val\n",
    "        net.eval()\n",
    "        full_path = []\n",
    "        with torch.no_grad():\n",
    "            for step, (img, label, _) in enumerate(val_dataloader):\n",
    "                img = img.float().to(device)\n",
    "                label = label.long().to(device)\n",
    "                out = net(img)\n",
    "                loss = loss_fcn(out, label)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                label = label.cpu().detach()\n",
    "                out = out.cpu().detach()\n",
    "                val_y_true, val_y_pred = UT.assemble_labels(step, val_y_true, val_y_pred, label, out)\n",
    "                \n",
    "                for item in _:\n",
    "                    full_path.append(item)\n",
    "                \n",
    "        val_loss = val_loss/(step+1)\n",
    "        val_acc = float(torch.sum(torch.max(val_y_pred, 1)[1]==val_y_true))/ float(len(val_y_pred))\n",
    "        val_auc = metrics.roc_auc_score(val_y_true, val_y_pred[:,1])\n",
    "        val_f1 = metrics.f1_score(val_y_true, torch.max(val_y_pred, 1)[1])\n",
    "        val_precision = metrics.precision_score(val_y_true, torch.max(val_y_pred, 1)[1])\n",
    "        val_recall = metrics.recall_score(val_y_true, torch.max(val_y_pred, 1)[1])\n",
    "        val_ap = metrics.average_precision_score(val_y_true, torch.max(val_y_pred, 1)[1]) #average_precision\n",
    "\n",
    "\n",
    "        train_hist.append([train_loss, acc, auc, f1, precision, recall, ap])\n",
    "        val_hist.append([val_loss, val_acc, val_auc, val_f1, val_precision, val_recall, val_ap])             \n",
    "\n",
    "        t.set_description(\"Epoch: %i, train loss: %.4f, train acc: %.4f, val loss: %.4f, val acc: %.4f, test acc: %.4f\" \n",
    "                          %(e, train_loss, acc, val_loss, val_acc, test_acc))\n",
    "\n",
    "\n",
    "        if(old_acc<val_acc):\n",
    "            old_acc = val_acc\n",
    "            old_auc = val_auc\n",
    "            best_epoch = e\n",
    "            test_loss = 0\n",
    "            test_y_true = val_y_true\n",
    "            test_y_pred = val_y_pred            \n",
    "\n",
    "            test_loss = val_loss\n",
    "            test_acc = float(torch.sum(torch.max(test_y_pred, 1)[1]==test_y_true))/ float(len(test_y_pred))\n",
    "            test_auc = metrics.roc_auc_score(test_y_true, test_y_pred[:,1])\n",
    "            test_f1 = metrics.f1_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_precision = metrics.precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_recall = metrics.recall_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_ap = metrics.average_precision_score(test_y_true, torch.max(test_y_pred, 1)[1]) #average_precision\n",
    "\n",
    "            test_performance = [best_epoch, test_loss, test_acc, test_auc, test_f1, test_precision, test_recall, test_ap]\n",
    "        \n",
    "        if(old_acc==val_acc) and (old_auc<val_auc):\n",
    "            old_acc = val_acc\n",
    "            old_auc = val_auc\n",
    "            best_epoch = e\n",
    "            test_loss = 0\n",
    "            test_y_true = val_y_true\n",
    "            test_y_pred = val_y_pred            \n",
    "\n",
    "            test_loss = val_loss\n",
    "            test_acc = float(torch.sum(torch.max(test_y_pred, 1)[1]==test_y_true))/ float(len(test_y_pred))\n",
    "            test_auc = metrics.roc_auc_score(test_y_true, test_y_pred[:,1])\n",
    "            test_f1 = metrics.f1_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_precision = metrics.precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_recall = metrics.recall_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "            test_ap = metrics.average_precision_score(test_y_true, torch.max(test_y_pred, 1)[1]) #average_precision\n",
    "\n",
    "            test_performance = [best_epoch, test_loss, test_acc, test_auc, test_f1, test_precision, test_recall, test_ap]\n",
    "    return train_hist, val_hist, test_performance, test_y_true, test_y_pred, full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_PATH = '/home/raytrack/.jupyter/Dynamic/Preprocessed'\n",
    "\n",
    "\n",
    "GPU = 0\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "EPOCHS = 150\n",
    "\n",
    "LR = 0.00001\n",
    "LOSS_WEIGHTS = torch.tensor([1., 1.]) \n",
    "\n",
    "device = torch.device('cuda:'+str(GPU) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 149, train loss: 0.0000, train acc: 1.0000, val loss: 0.3391, val acc: 0.8947, test acc: 1.0000: 100%|█| 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 149, train loss: 0.0000, train acc: 1.0000, val loss: 0.9524, val acc: 0.8824, test acc: 0.9412: 100%|█| 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/raytrack/.conda/envs/torch1121/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 149, train loss: 0.0000, train acc: 1.0000, val loss: 0.3123, val acc: 0.8571, test acc: 0.9524: 100%|█| 150"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32, 0.12782560250489042, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [47, 0.4907944569650782, 0.9411764705882353, 0.9545454545454546, 0.9523809523809523, 1.0, 0.9090909090909091, 0.9679144385026738], [51, 0.2946976402140122, 0.9523809523809523, 0.9818181818181818, 0.9565217391304348, 0.9166666666666666, 1.0, 0.9166666666666666]]\n",
      "ACC 0.9649, AUC 0.9716, F1 0.9667, Prec 0.9667, Recall 0.9667, AP 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#DATA_PATH = '/data/scratch/gliang/data/adni/ADNI2_MRI_Feature/Alex_Layer-9_DynamicImage'\n",
    "#FEATURE_SHAPE=(256,5,5)\n",
    "#print('DATA_PATH:',DATA_PATH)\n",
    "\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "test_performance = []\n",
    "test_y_true = np.asarray([])\n",
    "test_y_pred = np.asarray([])\n",
    "full_path = np.asarray([])\n",
    "for i in range(0, 3):\n",
    "    print('Train Fold', i)\n",
    "    \n",
    "    TEST_NUM = i\n",
    "    TRAIN_LABEL, TEST_LABEL = prep_data(LABEL_PATH, TEST_NUM)\n",
    "    \n",
    "    train_dataset = Dataset_Early_Fusion(label_file=TRAIN_LABEL)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=BATCH_SIZE , shuffle=True, drop_last=False)\n",
    "\n",
    "    val_dataset = Dataset_Early_Fusion(label_file=TEST_LABEL)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, num_workers=0, batch_size=BATCH_SIZE , shuffle=False, drop_last=False)\n",
    "        \n",
    "    cur_result = train(train_dataloader, val_dataloader)\n",
    "    \n",
    "    train_hist.append(cur_result[0])\n",
    "    val_hist.append(cur_result[1]) \n",
    "    test_performance.append(cur_result[2]) \n",
    "    test_y_true = np.concatenate((test_y_true, cur_result[3].numpy()))\n",
    "    if(len(test_y_pred) == 0):\n",
    "        test_y_pred = cur_result[4].numpy()\n",
    "    else:\n",
    "        test_y_pred = np.vstack((test_y_pred, cur_result[4].numpy()))\n",
    "    full_path = np.concatenate((full_path, np.asarray(cur_result[5])))\n",
    "\n",
    "print(test_performance)\n",
    "\n",
    "test_y_true = torch.tensor(test_y_true)\n",
    "test_y_pred = torch.tensor(test_y_pred)\n",
    "test_acc = float(torch.sum(torch.max(test_y_pred, 1)[1]==test_y_true.long()))/ float(len(test_y_pred))\n",
    "test_auc = metrics.roc_auc_score(test_y_true, test_y_pred[:,1])\n",
    "test_f1 = metrics.f1_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "test_precision = metrics.precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "test_recall = metrics.recall_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "test_ap = metrics.average_precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "\n",
    "print('ACC %.4f, AUC %.4f, F1 %.4f, Prec %.4f, Recall %.4f, AP %.4f' \n",
    "      %(test_acc, test_auc, test_f1, test_precision, test_recall, test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
